{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fcrmhmd/K-Nearest-Neighbors/blob/main/KNN_Keluhan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S9kUa77Gxb8"
      },
      "source": [
        "# IMPORT & LOAD LIB DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z24fKgOb4ljm",
        "outputId": "f9be842e-7b6d-4a05-8b94-5422c3a9f5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 61 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 81 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 102 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 112 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 122 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 133 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 143 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 153 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 163 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 174 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 184 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 194 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 204 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 209 kB 5.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n",
            "Collecting fsspec>=0.3.3\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "Successfully installed fsspec-2022.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Sastrawi\n",
        "!pip install 'fsspec>=0.3.3'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "sHH9uRutwhIF",
        "outputId": "384b96a9-8ff2-42a2-94ea-a33d3ddf4ebf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d8932ee7961f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0memo_unicode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUNICODE_EMO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMOTICONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emo_unicode'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "import time\n",
        "import dask.dataframe as dd\n",
        "from dask.multiprocessing import get"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpiIcmocGkBn"
      },
      "source": [
        "# PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoWXt_Ov06lR"
      },
      "outputs": [],
      "source": [
        "class TextPreprocessing:\n",
        "    def __init__(self, text=\"test\"):\n",
        "        self.text = text\n",
        "\n",
        "    def lowercase(self):\n",
        "        \"\"\"Convert to lowercase\"\"\"\n",
        "        self.text = str(self.text).lower()\n",
        "        self.text = self.text.strip()\n",
        "        return self\n",
        "\n",
        "    def strip_html(self):\n",
        "        \"\"\"Stopword removal\"\"\"\n",
        "        soup = BeautifulSoup(self.text, \"html.parser\")\n",
        "        self.text = soup.get_text()\n",
        "        return self\n",
        "\n",
        "    def remove_url(self):\n",
        "        \"\"\"Remove URL (http/https/www) or custom URL\"\"\"\n",
        "        self.text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", self.text)\n",
        "        return self\n",
        "\n",
        "    def remove_email(self):\n",
        "        \"\"\"Remove email\"\"\"\n",
        "        self.text = re.sub(\"\\S*@\\S*\\s?\", \"\", self.text)\n",
        "        return self\n",
        "\n",
        "    def remove_between_square_brackets(self):\n",
        "        \"\"\"Remove string beetwen square brackets []\"\"\"\n",
        "        self.text = re.sub(\"\\[[^]]*\\]\", \"\", self.text)\n",
        "        return self\n",
        "\n",
        "    def remove_numbers(self):\n",
        "        \"\"\"Remove numbers\"\"\"\n",
        "        self.text = re.sub(\"[-+]?[0-9]+\", \"\", self.text)\n",
        "        return self\n",
        "\n",
        "    def remove_emoji(self):\n",
        "        \"\"\"Remove emoji, e.g 😜😀 \"\"\"\n",
        "        emoji_pattern = re.compile(\n",
        "            \"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "            \"]+\",\n",
        "            flags=re.UNICODE,\n",
        "        )\n",
        "        self.text = emoji_pattern.sub(r\"\", self.text)\n",
        "        return self\n",
        "\n",
        "    def remove_emoticon(self):\n",
        "        \"\"\"Remove emoticon, e.g :-)\"\"\"\n",
        "        emoticon_pattern = re.compile(u\"(\" + u\"|\".join(k for k in EMOTICONS) + u\")\")\n",
        "        self.text = emoticon_pattern.sub(r\"\", self.text)\n",
        "        return self\n",
        "    \n",
        "    ##def convert_emoji(self):\n",
        "    ##    \"\"\"Convert emoji to word\"\"\"\n",
        "    ##    for emoji in UNICODE_EMO:\n",
        "    ##        self.text = self.text.replace(\n",
        "    ##            emoji,\n",
        "    ##            \"_\".join(UNICODE_EMO[emoji].replace(\",\", \"\").replace(\":\", \"\").split()),\n",
        "    ##        )\n",
        "    ##    return self\n",
        "\n",
        "    ##def convert_emoticon(self):\n",
        "    ##    \"\"\"Convert emoticon to word\"\"\"\n",
        "    ##    for emoticon in EMOTICONS:\n",
        "    ##        self.text = re.sub(\n",
        "    ##            u\"(\" + emoticon + \")\",\n",
        "    ##            \"_\".join(EMOTICONS[emoticon].replace(\",\", \"\").split()),\n",
        "    ##            self.text,\n",
        "    ##        )\n",
        "    ##    return self\n",
        "\n",
        "    def remove_punctuation(self):\n",
        "        \"\"\"Remove punctuation\"\"\"\n",
        "        self.text = re.sub(r\"[^\\w\\s]\", \"\", self.text)\n",
        "        return self\n",
        "\n",
        "    def remove_non_ascii(self):\n",
        "        \"\"\"Remove non-ascii character\"\"\"\n",
        "        self.text = (\n",
        "            unicodedata.normalize(\"NFKD\", self.text)\n",
        "            .encode(\"ascii\", \"ignore\")\n",
        "            .decode(\"utf-8\", \"ignore\")\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def normalize_word(self):\n",
        "        \"\"\"Normalize slang world\"\"\"\n",
        "        normal_word_path = pd.read_csv(\"key_norm.csv\")\n",
        "\n",
        "        self.text = \" \".join(\n",
        "            [\n",
        "                normal_word_path[normal_word_path[\"singkat\"] == word][\"hasil\"].values[0]\n",
        "                if (normal_word_path[\"singkat\"] == word).any()\n",
        "                else word\n",
        "                for word in self.text.split()\n",
        "            ]\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def stemming(self):\n",
        "        \"\"\"Stemming for Bahasa with Sastrawi\"\"\"\n",
        "        factory = StemmerFactory()\n",
        "        stemmer = factory.create_stemmer()\n",
        "\n",
        "        self.text = stemmer.stem(self.text)\n",
        "        return self\n",
        "\n",
        "    def tokenize(self):\n",
        "        \"\"\"Tokenize words\"\"\"\n",
        "        self.words = nltk.word_tokenize(self.text)\n",
        "        return self\n",
        "\n",
        "    def stopwords_removal(self):\n",
        "        \"\"\"Stopword removal\"\"\"\n",
        "        stopword = stopwords.words(\"indonesian\")\n",
        "        more_stopword = [\n",
        "            \"assalamualaikum\", \"wr\", \"wb\", \"pak\",\n",
        "            \"bu\", \"selamat\", \"siang\", \"pagi\",\n",
        "            \"sore\", \"malam\", \"saya\",\n",
        "            \"terimakasih\", \"terima\",\n",
        "            \"kasih\", \"kepada\", \"bpk\",\n",
        "            \"ibu\", \"mohon\", \"tolong\",\n",
        "            \"maaf\", \"dear\", \"wassalamualaikum\",\n",
        "            \"regards\", \"nbsp\", \"amp\", \"lg\", \"lgi\", \"kak\",\n",
        "            \"bapakibu\",\"bapak\", \"admin\", ]  # add more stopword to default corpus\n",
        "        stop_factory = stopword + more_stopword\n",
        "        stop_factory.remove('tak')\n",
        "        stop_factory.remove('akhir')\n",
        "        \n",
        "        clean_words = []\n",
        "        for word in self.words:\n",
        "            if word not in stop_factory:\n",
        "                clean_words.append(word)\n",
        "        self.words = clean_words  \n",
        "        return self\n",
        "\n",
        "    def join_words(self):\n",
        "        \"\"\"Join all words\"\"\"\n",
        "        self.words = \" \".join(self.words)\n",
        "        return self\n",
        "    \n",
        "    def do_all(self, text):\n",
        "        \"\"\"Do all text preprocessing process\"\"\" \n",
        "        self.text = text\n",
        "        self = self.lowercase()\n",
        "        self = self.strip_html()\n",
        "        self = self.remove_url()\n",
        "        self = self.remove_email()\n",
        "        self = self.remove_between_square_brackets()\n",
        "        self = self.remove_numbers()\n",
        "        self = self.remove_emoticon()\n",
        "        self = self.remove_emoji()\n",
        "        ##self = self.convert_emoticon()\n",
        "        ##self = self.convert_emoji()\n",
        "        self = self.remove_punctuation()\n",
        "        self = self.remove_non_ascii()\n",
        "        self = self.normalize_word()\n",
        "        self = self.stemming()\n",
        "        self = self.tokenize()\n",
        "        self = self.stopwords_removal()\n",
        "        self = self.join_words()\n",
        "        return self.words\n",
        "\n",
        "def dask_this(data):\n",
        "    data['clean_keluhan'] = data['keluhan'].apply(tp.do_all)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "9rCMcPfxy3VH",
        "outputId": "bab5df92-9fb6-4b1a-f1ad-591238f73809"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>keluhan</th>\n",
              "      <th>bagian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dear BAA,Mohon di bantu untuk merubah satatus ...</td>\n",
              "      <td>BAA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Persyaratan untuk pembuatan ktm terlalu rumit</td>\n",
              "      <td>BAA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Biaya pembuatan ktm yang terlalu mahal</td>\n",
              "      <td>BAA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>warna dan tulisan pada ktm mudah pudar</td>\n",
              "      <td>BAA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tolong saya sudah 2 kali ke bagian kemahasiswa...</td>\n",
              "      <td>BAA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1600</th>\n",
              "      <td>saat pemiihan kantor saat magang ada berapa ge...</td>\n",
              "      <td>LAA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1601</th>\n",
              "      <td>min sayaa sudah mengambil kantor magang di bal...</td>\n",
              "      <td>LAA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602</th>\n",
              "      <td>magang nantinya bisa di tempatkan di kantor ya...</td>\n",
              "      <td>LAA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1603</th>\n",
              "      <td>min saya mau pindah kantor KP saya tidak betah...</td>\n",
              "      <td>LAA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1604</th>\n",
              "      <td>min saya mau pindah kantor magang saya tidak b...</td>\n",
              "      <td>LAA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1605 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                keluhan bagian\n",
              "0     Dear BAA,Mohon di bantu untuk merubah satatus ...    BAA\n",
              "1         Persyaratan untuk pembuatan ktm terlalu rumit    BAA\n",
              "2                Biaya pembuatan ktm yang terlalu mahal    BAA\n",
              "3                warna dan tulisan pada ktm mudah pudar    BAA\n",
              "4     Tolong saya sudah 2 kali ke bagian kemahasiswa...    BAA\n",
              "...                                                 ...    ...\n",
              "1600  saat pemiihan kantor saat magang ada berapa ge...    LAA\n",
              "1601  min sayaa sudah mengambil kantor magang di bal...    LAA\n",
              "1602  magang nantinya bisa di tempatkan di kantor ya...    LAA\n",
              "1603  min saya mau pindah kantor KP saya tidak betah...    LAA\n",
              "1604  min saya mau pindah kantor magang saya tidak b...    LAA\n",
              "\n",
              "[1605 rows x 2 columns]"
            ]
          },
          "execution_count": 84,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_sheet = ['Akademik', 'PSAL', 'PuTI', 'Logistik', 'Keuangan', 'Pengelolaan Mahasiswa', 'Pengembangan Karir, Alumni dan ', 'Laboran', 'Kemahasiswaan', 'LAA']\n",
        "\n",
        "data_keluhan = []\n",
        "data_bagian = []\n",
        "for s in list_sheet:\n",
        "  \n",
        "  data_append = pd.read_excel(open('daftar bagian.xlsx', 'rb'),\n",
        "                sheet_name=str(s))\n",
        "  data_keluhan += data_append['KELUHAN'].tolist()\n",
        "  data_bagian+= data_append['BAGIAN'].tolist()\n",
        "  # data.append([data_append['KELUHAN'].tolist(), data_append['BAGIAN'].tolist()])\n",
        "  \n",
        "\n",
        "data = pd.DataFrame({\"keluhan\": data_keluhan, \"bagian\": data_bagian})\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHYv_DqD_I0_"
      },
      "outputs": [],
      "source": [
        "tp = TextPreprocessing() # load module text preprocessing  \n",
        "ddata = dd.from_pandas(data, npartitions=10)\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "  start_time = time.time()\n",
        "  data = ddata.map_partitions(dask_this).compute(scheduler='processes', num_workers=10)\n",
        "except:\n",
        "    print('Text preprocessing failed !')\n",
        "else:\n",
        "    data.to_csv('clean_data_training.csv', encoding='utf-8')\n",
        "    print('Text preprocessing success !')\n",
        "    print('Elapsed time:', time.time() - start_time, 'seconds')\n",
        "finally:\n",
        "    print('\\nFinish')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dibaawha ini asal semuaa"
      ],
      "metadata": {
        "id": "gH9QI0Tf_lcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tp = TextPreprocessing() # load module text preprocessing  \n",
        "ddata = dd.from_pandas(data, npartitions=10)\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "  start_time = time.time()\n",
        "  data = ddata.map_partitions(dask_this).compute(scheduler='processes', num_workers=10)\n",
        "except:\n",
        "    print('Text preprocessing failed !')\n",
        "else:\n",
        "    data.to_csv('clean_data_training.csv', encoding='utf-8')\n",
        "    print('Text preprocessing success !')\n",
        "    print('Elapsed time:', time.time() - start_time, 'seconds')\n",
        "finally:\n",
        "    print('\\nFinish')"
      ],
      "metadata": {
        "id": "t0pethTM_5-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  start_time = time.time()\n",
        "  data = ddata.map_partitions(dask_this).compute(scheeduler='processes', num_workers=10)"
      ],
      "metadata": {
        "id": "-CTScPVVAMjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QCTfiA6cAMdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VlNA_g2xAMUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TR8Cy3LiAMNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V97FcVWfAMKr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "KNN_Keluhan.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}